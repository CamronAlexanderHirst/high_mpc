---
layout: default
pagination:
enabled: true
---
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<div class="home">

  <div class="site-header-container {% if site.cover %}has-cover{% endif %}"
    {% if site.cover %}style="background-image: url({{ site.cover | prepend: site.baseurl }});" {% endif %}>
    <div class="scrim {% if site.cover %}has-cover{% endif %}">
      <header class="site-header">
        <h1 class="title" style=font-size:80px>{{ site.title }}</h1>
        {% if site.subtitle %}<p class="title" style=font-size:30px>{{ site.subtitle }}</p>{% endif %}
      </header>
    </div>
  </div>

  <div class="wrapper">
    <h1 id="headings">Introduction</h1>
    <p style="text-align: justify">
      This work establishes a novel architecture to bridge the gap between the
      learning-based method of <b>policy search</b> and the optimization-based approach
      of <b>model predictive control (MPC)</b>.
      Our main contribution is a policy search for MPC framework, in which
      the design of both fixed and adaptive high-level decision variables
      for the MPC controller is formulated as a policy search problem.
      We use this framework to learn three kinds of high-level policies:
      a Gaussian policy, a Gaussian linear policy, and a neural network policy.
      We show that the policy search algorithm
      allows us to learn both state-independent and state-dependent
      decision variables for MPC and the training performs consistently well
      across different random seeds.

      <br>
      <br>

      We apply the proposed method to address one of the major
      challenges towards agile drone flight in highly dynamic
      environments--flying a quadrotor through fast-moving gates.
      The resulting controller called <b>High-MPC</b>, which consists of a
      high-level neural network policy and a MPC controller, achieved adaptive and robust
      real-time control performance for the task, in both simulation and the real world.
      The key is using the neural network for selecting the desired traversal time for the MPC
      and the MPC for simultaneously trajectory planning and vehicle control.
      These findings suggest that our framework opens up new opportunities for
      achieving adaptive optimal control for the physical robots in a
      highly dynamic environment.

    </p>

    <!-- <h2 id="headings">Video</h2>
    <iframe width="750" height="421" src="https://www.youtube.com/embed/Zeyv1bN9v4A" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowfullscreen></iframe> -->

  </div>
</div>

<!-- <div class="container" style="margin-top:30px;margin-bottom:30px;">
      <h2>Reference</h2>
      If you use this code in a publication, please cite our paper.
      <div class="row">

        <div class="col-lg-4" style="padding:0;">
          <a href="https://arxiv.org/abs/2009.00563">
            <img style="width: 300px" src="assets/paper_thumbnail.png" alt="Flightmare: A Flexible Quadrotor Simulator">
          </a>
        </div>

        <div class="col-lg-8" style="padding:0;">
          <div style="background:#ffffff;margin:0px;padding:0px;">
            <pre>
          <code class="pre-scrollable" style="background:#ffffff;color:#333;font-size:12px;padding:0px;border-width:0px;">
            @article{yunlong2020flightmare,
            title={Flightmare: A Flexible Quadrotor Simulator},
            author={Song, Yunlong and Naji, Selim and Kaufmann, Elia and Loquercio, Antonio and Scaramuzza, Davide},
            journal={arXiv preprint arXiv:2009.00563},
            year={2020}}
          </code>
          </pre>
          </div>
        </div>
      </div>
    </div> -->